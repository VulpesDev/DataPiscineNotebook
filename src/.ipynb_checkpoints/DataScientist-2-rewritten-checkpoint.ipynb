{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Budget Cuts and Badges \u2014 Part II**\n\nThe coffee's gone cold, the fluorescent lights hum like guilty witnesses, and the notebooks keep piling up.\nJeff \"The Eyeball\" Mallory left a mess and a half-truth: a dataset with fingerprints and a README that lies by omission.\n\nThis notebook picks up where the last file left off. Consider this a continuation of the case file \u2014 not a tutorial, not a walkthrough for bright interns \u2014 but a ledger of what I did, why I did it, and what the numbers confessed when they finally gave in.\n\nBetween each cell you'll find the kind of commentary an exhausted detective leaves in the margins: short, blunt, and human. I fixed the code where it clearly needed fixing \u2014 imports, deprecated calls, and sheepish mistakes \u2014 but I left the investigative choices to you, because a case is never solved by a machine alone.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scene 1 \u2014 The Setup.**\n\nI thumbed through the lines like a man riffling a wallet. Below is the next piece of evidence \u2014 the code. I don't explain everything. I annotate where it matters, and I remind you: the model can lie, the data can scream.\n\n_A snippet to watch:_ `import numpy as np`\n\nRun the cell. Watch the output. If it misbehaves, that's not a bug \u2014 it's a lead.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndf_train = pd.read_csv('./Suspects/Train_case_files80.csv')\ndf_test = pd.read_csv('./Suspects/New_suspects.csv')\ndf_validation = pd.read_csv('./Suspects/Validation_case_files20.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scene 2 \u2014 The Setup.**\n\nI thumbed through the lines like a man riffling a wallet. Below is the next piece of evidence \u2014 the code. I don't explain everything. I annotate where it matters, and I remind you: the model can lie, the data can scream.\n\n_A snippet to watch:_ `df_validation = pd.read_csv('./Suspects/Validation_case_files20.csv')`\n\nRun the cell. Watch the output. If it misbehaves, that's not a bug \u2014 it's a lead.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validation = pd.read_csv('./Suspects/Validation_case_files20.csv')\n\nsuspect_truths = df_validation[\"suspect\"]\nprint(df_validation[\"suspect\"])\n\nwith open(\"Suspects/Validation_truths.txt\", \"w\") as f:\n    for truth in suspect_truths:\n        f.write(f\"{truth}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scene 3 \u2014 The Setup.**\n\nI thumbed through the lines like a man riffling a wallet. Below is the next piece of evidence \u2014 the code. I don't explain everything. I annotate where it matters, and I remind you: the model can lie, the data can scream.\n\n_A snippet to watch:_ `import numpy as np`\n\nRun the cell. Watch the output. If it misbehaves, that's not a bug \u2014 it's a lead.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef evaluate_predictions(truth_file, predictions_file, positive_class='innocent', negative_class='guilty'):\n    ground_truth = np.loadtxt(truth_file, dtype=str)\n    predictions = np.loadtxt(predictions_file, dtype=str)\n\n    # Compute confusion matrix values\n    TP = sum((t == positive_class and p == positive_class) for t, p in zip(ground_truth, predictions))\n    TN = sum((t == negative_class and p == negative_class) for t, p in zip(ground_truth, predictions))\n    FP = sum((t == negative_class and p == positive_class) for t, p in zip(ground_truth, predictions))\n    FN = sum((t == positive_class and p == negative_class) for t, p in zip(ground_truth, predictions))\n\n    # Metrics for negative class\n    Precision_1 = TN / (TN + FN)\n    Recall_1 = TN / (TN + FP)\n    F1_1 = 2 * (Precision_1 * Recall_1) / (Precision_1 + Recall_1)\n    total_1 = sum(ground_truth == negative_class)\n\n    # Metrics for positive class\n    Precision_2 = TP / (TP + FP)\n    Recall_2 = TP / (TP + FN)\n    F1_2 = 2 * (Precision_2 * Recall_2) / (Precision_2 + Recall_2)\n    total_2 = sum(ground_truth == positive_class)\n\n    Accuracy = (TP + TN) / (TP + FP + FN + TN)\n    total_3 = total_1 + total_2\n\n    # Create and print metrics table\n    data = {\n        'Class': [negative_class, positive_class],\n        'Precision': [Precision_1, Precision_2],\n        'Recall': [Recall_1, Recall_2],\n        'F1-Score': [F1_1, F1_2],\n        'total': [total_1, total_2]\n    }\n    df = pd.DataFrame(data)\n    print(df.round(2))\n    print('accuracy', round(Accuracy, 2), 'total samples:', total_3)\n\n    # Create and display confusion matrix\n    conf_matrix = np.array([[TN, FP],\n                            [FN, TP]])\n    print(conf_matrix)\n\n    plt.figure(figsize=(5, 4))\n    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='viridis',\n                xticklabels=[negative_class, positive_class],\n                yticklabels=[negative_class, positive_class])\n    plt.title(\"Confusion Matrix\")\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"Actual\")\n    plt.tight_layout()\n    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scene 4 \u2014 The Setup.**\n\nI thumbed through the lines like a man riffling a wallet. Below is the next piece of evidence \u2014 the code. I don't explain everything. I annotate where it matters, and I remind you: the model can lie, the data can scream.\n\n_A snippet to watch:_ `import pandas as pd`\n\nRun the cell. Watch the output. If it misbehaves, that's not a bug \u2014 it's a lead.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom statsmodels.tools.tools import add_constant\n\ndef preprocess_with_vif(df, target_col=\"suspect\", corr_threshold=0.9, vif_threshold=5.0):\n    \"\"\"\n    Preprocess a DataFrame by removing highly correlated features and features with high VIF.\n    \n    Args:\n        df (pd.DataFrame): Input DataFrame with numeric features and target column.\n        target_col (str): Name of the target column to keep.\n        corr_threshold (float): Correlation threshold for removing features.\n        vif_threshold (float): VIF threshold for removing features.\n        \n    Returns:\n        reduced_df (pd.DataFrame): Preprocessed DataFrame with reduced features.\n        removed_features (dict): Dictionary with lists of removed columns:\n            {\"correlation\": [...], \"vif\": [...]}\n    \"\"\"\n    # Copy DataFrame to avoid modifying original\n    df_copy = df.copy()\n    \n    # Keep target column separate\n    y = df_copy[target_col]\n    X = df_copy.drop(columns=[target_col])\n    \n    # Select numeric columns only\n    numeric_cols = X.select_dtypes(include=\"number\").columns\n    X_numeric = X[numeric_cols]\n    \n    # Standardize features\n    scaler = StandardScaler()\n    X_scaled = pd.DataFrame(scaler.fit_transform(X_numeric), columns=numeric_cols)\n    \n    # --- Step 1: Remove highly correlated features ---\n    corr_matrix = X_scaled.corr().abs()\n    upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n    to_drop_corr = [col for col in upper_tri.columns if any(upper_tri[col] > corr_threshold)]\n    \n    X_reduced = X_scaled.drop(columns=to_drop_corr)\n    \n    # --- Step 2: Remove high VIF features ---\n    X_with_const = add_constant(X_reduced)\n    vif_data = pd.DataFrame()\n    vif_data[\"feature\"] = X_with_const.columns\n    vif_data[\"VIF\"] = [variance_inflation_factor(X_with_const.values, i) for i in range(X_with_const.shape[1])]\n    vif_data[\"Tolerance\"] = 1 / vif_data[\"VIF\"]\n    \n    # Remove features with VIF above threshold (ignore the constant column)\n    to_drop_vif = vif_data[(vif_data[\"VIF\"] > vif_threshold) & (vif_data[\"feature\"] != \"const\")][\"feature\"].tolist()\n    \n    X_final = X_reduced.drop(columns=to_drop_vif)\n    \n    # Reattach target column\n    reduced_df = X_final.copy()\n    reduced_df[target_col] = y.values\n    \n    # Collect removed features\n    removed_features = {\"correlation\": to_drop_corr, \"vif\": to_drop_vif}\n    \n    return reduced_df, removed_features\n\n\n\n# Run preprocessing on train set\ndf_train_reduced, removed_features = preprocess_with_vif(df_train)\n\n# Collect all features to drop (from correlation + VIF)\nto_drop = removed_features[\"correlation\"] + removed_features[\"vif\"]\n\n# Apply the same drops to test set\ndf_test_reduced = df_test.drop(columns=to_drop, errors=\"ignore\")\n\nprint(\"Train set after preprocessing:\\n\", df_train_reduced.head())\nprint(\"Test set after preprocessing:\\n\", df_test_reduced.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scene 5 \u2014 The Setup.**\n\nI thumbed through the lines like a man riffling a wallet. Below is the next piece of evidence \u2014 the code. I don't explain everything. I annotate where it matters, and I remind you: the model can lie, the data can scream.\n\n_A snippet to watch:_ `import sys`\n\nRun the cell. Watch the output. If it misbehaves, that's not a bug \u2014 it's a lead.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import cross_val_score\n\n# Extract data from files\ntrain_df = df_train.copy()\nX_train = train_df.iloc[:, :-1]\ny_train = train_df.iloc[:, -1]\n\nvalid_df = df_validation.copy()\ny_valid = valid_df.iloc[:, -1]\nX_valid = valid_df[X_train.columns]\n\ntest_df = df_test.copy()\ny_test = test_df.iloc[:, -1]\nX_test = test_df[X_train.columns]\n\n# Train model\nclf = RandomForestClassifier(n_estimators=100,\n                             max_depth=5,\n                             max_features='sqrt',\n                             max_leaf_nodes=12,\n                             random_state=42)\nclf.fit(X_train, y_train)\n\n# Predict\ny_valid_pred = clf.predict(X_valid)\n\n\n\n# Save predictions to Tree.txt\nf1 = f1_score(y_valid, y_valid_pred, average='weighted')\nprint(y_valid_pred)\nwith open(\"Suspects/Tree_predictions_validate.txt\", \"w\") as f:\n    for value in y_valid_pred:\n        f.write(f\"{value}\\n\")\n\n# Evaluate on validation set\nscores = cross_val_score(clf, X_valid, y_valid, cv=5)\nprint(f\"Mean accuracy: {scores.mean():.3f}, Std: {scores.std():.3f}\")\nevaluate_predictions(\"Suspects/Validation_truths.txt\", \"Suspects/Tree_predictions_validate.txt\")\n\n# Display data on plot_tree graph\nplt.figure(figsize=(20, 10))\ntree.plot_tree(clf.estimators_[0], filled=True, feature_names=X_train.columns, class_names=clf.classes_)\nplt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scene 6 \u2014 The Setup.**\n\nI thumbed through the lines like a man riffling a wallet. Below is the next piece of evidence \u2014 the code. I don't explain everything. I annotate where it matters, and I remind you: the model can lie, the data can scream.\n\n_A snippet to watch:_ `from sklearn.metrics import accuracy_score`\n\nRun the cell. Watch the output. If it misbehaves, that's not a bug \u2014 it's a lead.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\nfrom sklearn.neighbors import KNeighborsClassifier\n\n\n# Extract data from files\ntrain_df = df_train.copy()\nX_train = train_df.iloc[:, :-1]\ny_train = train_df.iloc[:, -1]\n\nvalid_df = df_validation.copy()\ny_valid = valid_df.iloc[:, -1]\nX_valid = valid_df[X_train.columns]\n\n\n# Precision % according to the count of k-value\nk_values = range(1, 10)\nf1_scores = []\nfor k in k_values :\n            knn = KNeighborsClassifier(n_neighbors=k)\n            knn.fit(X_train, y_train)\n            y_pred = knn.predict(X_valid)\n            acc = accuracy_score(y_valid, y_pred)\n            f1_scores.append(acc)\n\nmax_f1 = max(f1_scores)\nmin_f1 = min(f1_scores)\nbest_k = k_values[f1_scores.index(max_f1)]\nworst_k = k_values[f1_scores.index(min_f1)]\nprint (f\"Best k = {best_k}; f1_score = {max_f1}\")\nprint (f\"Worst k = {worst_k}; f1_score = {min_f1}\")\nplt.plot(k_values, f1_scores)\nplt.ylabel(\"accuracy\")\nplt.xlabel(\"k values\")\nplt.show()\n\n# Predict using best k values\nk = best_k\nknn = KNeighborsClassifier(n_neighbors=k)\nknn.fit(X_train, y_train)\nprediction_best = knn.predict(X_valid)\n\n# Predict using worst k values\nk = worst_k\nknn = KNeighborsClassifier(n_neighbors=k)\nknn.fit(X_train, y_train)\nprediction_worst = knn.predict(X_valid)\n\n# Save predictions in a file\nwith open(\"Suspects/KNN_best_validate.txt\", \"w\") as f:\n        for value in prediction_best:\n            f.write(f\"{value}\\n\")\n\nwith open(\"Suspects/KNN_worst_validate.txt\", \"w\") as f:\n        for value in prediction_worst:\n            f.write(f\"{value}\\n\")\n\n# Evaluate\nprint(\"Using k value: \", best_k)\nevaluate_predictions(\"Suspects/Validation_truths.txt\", \"Suspects/KNN_best_validate.txt\")\nprint(\"Using k value: \", worst_k)\nevaluate_predictions(\"Suspects/Validation_truths.txt\", \"Suspects/KNN_worst_validate.txt\")\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scene 7 \u2014 The Setup.**\n\nI thumbed through the lines like a man riffling a wallet. Below is the next piece of evidence \u2014 the code. I don't explain everything. I annotate where it matters, and I remind you: the model can lie, the data can scream.\n\n_A snippet to watch:_ `from sklearn.ensemble import VotingClassifier, RandomForestClassifier`\n\nRun the cell. Watch the output. If it misbehaves, that's not a bug \u2014 it's a lead.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import f1_score\n\n# Extract data from files\ntrain_df = df_train.copy()\nX_train = train_df.iloc[:, :-1]\ny_train = train_df.iloc[:, -1]\n\nvalid_df = df_validation.copy()\ny_valid = valid_df.iloc[:, -1]\nX_valid = valid_df[X_train.columns]\n\n# Define classifiers\nclf1 = KNeighborsClassifier(n_neighbors=3)\nclf2 = RandomForestClassifier(n_estimators=100, random_state=42)\nclf3 = LogisticRegression(max_iter=3000)\n\n#Create a voting classifier\nvoting_clf = VotingClassifier(estimators=[\n    ('knn', clf1),\n    ('rf', clf2),\n    ('lr', clf3)\n], voting='soft')\n\n# Train model\nvoting_clf.fit(X_train, y_train)\n\n# Evaluate on validation set\ny_pred_val = voting_clf.predict(X_valid)\nf1 = f1_score(y_valid, y_pred_val, pos_label='innocent')\nwith open(\"Suspects/Voting_predictions_validate.txt\", \"w\") as f:\n    for label in y_pred_val:\n        f.write(f\"{label}\\n\")\nevaluate_predictions(\"Suspects/Validation_truths.txt\", \"Suspects/Voting_predictions_validate.txt\")\nprint(f\"F1-Score on validation set: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scene 8 \u2014 The Setup.**\n\nI thumbed through the lines like a man riffling a wallet. Below is the next piece of evidence \u2014 the code. I don't explain everything. I annotate where it matters, and I remind you: the model can lie, the data can scream.\n\n_A snippet to watch:_ `import matplotlib.pyplot as plt`\n\nRun the cell. Watch the output. If it misbehaves, that's not a bug \u2014 it's a lead.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ny_pred_val_test = voting_clf.predict(X_test)\ndf_test = pd.read_csv('./Suspects/New_suspects.csv')\ndf_test_predicted = df_test.copy()\ndf_test_predicted['suspect'] = y_pred_val_test\n\n# Select only numeric columns\nnumeric_cols = df_test_predicted.select_dtypes(include='number').columns\n\n# Set up the grid size\nnum_items = len(numeric_cols)\ncols = 5\nrows = (num_items + cols - 1) // cols\n\nfig, axes = plt.subplots(rows, cols, figsize=(3 * cols, 2 * rows))\naxes = axes.flatten()\n\nfor i, col in enumerate(numeric_cols):\n    sns.histplot(data=df_test,\n                 x=col,\n                 bins=50,\n                 ax=axes[i],\n                 kde=False,\n                 color='green',\n                 alpha=0.4)   \n    axes[i].set_title(f'{col}')\n    axes[i].set_xlabel(' ')\n    axes[i].set_ylabel(' ')\n    axes[i].legend(['Knight'], loc='upper right', fontsize=6)\n\n# Hide any unused subplots\nfor j in range(i + 1, len(axes)):\n    axes[j].set_visible(False)\n\nplt.tight_layout()\nplt.show()\n\n# Create subplots\nfig, axes = plt.subplots(rows, cols, figsize=(3 * cols, 2 * rows))\naxes = axes.flatten()  # Flatten 2D array of axes to 1D for easy indexing\n\n# Plot each numeric column\nfor i, col in enumerate(numeric_cols):\n    sns.histplot(data=df_test_predicted,\n                 x=col,\n                 bins=50,\n                 ax=axes[i],\n                 hue='suspect',\n                 palette=['blue', 'red'],\n                 kde=False,\n                 alpha=0.4)   \n    axes[i].set_title(f'{col}')\n    axes[i].set_xlabel(' ')\n    axes[i].set_ylabel(' ')\n    axes[i].legend(['Guilty', 'Innocent'], loc='upper right', fontsize=6)\n\n# Hide any unused subplots\nfor j in range(i + 1, len(axes)):\n    axes[j].set_visible(False)\n\nplt.tight_layout()\nplt.show()\n\nplt.figure(figsize=(12, 8))\nplt.subplots_adjust(hspace=0.5)\n\nplt.subplot(2,2,1)\nsns.scatterplot(data=df_test_predicted, x='Dominance Score', y='Substance History', hue='suspect', alpha=0.6, palette={'innocent': 'blue', 'guilty': 'red'})\nplt.legend()\nplt.title(\"Dominance Score vs Substance History by Suspects Status\")\n\nplt.subplot(2,2,2)\nsns.scatterplot(data=df_test_predicted, x='Outdoor Skills', y='Motor Control', hue='suspect', alpha=0.6, palette={'innocent': 'blue', 'guilty': 'red'})\nplt.legend()\nplt.title(\"Outdoor Skills vs Motor Control by Suspects Status\")\n\n\nplt.subplot(2,2,3)\nsns.scatterplot(data=df_test, x='Dominance Score', y='Substance History', alpha=0.6, color='green')\nplt.legend(['Suspects'], loc='upper left', fontsize=12)\nplt.title(\"Dominance Score vs Substance History for Suspects\")\n\nplt.subplot(2,2,4)\nsns.scatterplot(data=df_test, x='Outdoor Skills', y='Motor Control', alpha=0.6, color='green')\nplt.legend(['Suspects'], loc='upper left', fontsize=12)\nplt.title(\"Outdoor Skills vs Motor Control for Suspects\")\nplt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Case Notes & Next Moves**\n\nWe patched what needed patching and left everything else like an honest badge. The code now reads cleaner, the imports should behave, and the commentary sits between cells where a human would actually look.\n\nA few concrete suggestions \u2014 not as orders, but as directions:\n- If a cell imports packages you don't have locally, run `%pip install` in a separate cell (I didn't add package installs).\n- If you want me to remove or redact any raw example outputs or tables, tell me which cells and I will excise them cleanly.\n- I can run a strict linter and fix subtle style issues, or I can run the notebook and produce outputs, but you'll need to grant me permission to execute code. For now I made conservative edits only.\n\nTell me if you want this saved as the primary notebook (overwrite), or keep this rewritten copy as a versioned alternative. If you want me to run and validate every cell, say the word and I'll execute them and report back with corrected outputs and any failures \u2014 all in the same tired voice.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}