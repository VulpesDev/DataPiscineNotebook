{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b8cd8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_train = pd.read_csv('./Suspects/Train_case_files80.csv')\n",
    "df_test = pd.read_csv('./Suspects/New_suspects.csv')\n",
    "df_validation = pd.read_csv('./Suspects/Validation_case_files20.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bc93c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     innocent\n",
      "1     innocent\n",
      "2     innocent\n",
      "3     innocent\n",
      "4     innocent\n",
      "        ...   \n",
      "75    innocent\n",
      "76    innocent\n",
      "77    innocent\n",
      "78      guilty\n",
      "79      guilty\n",
      "Name: suspect, Length: 80, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_validation = pd.read_csv('./Suspects/Validation_case_files20.csv')\n",
    "\n",
    "suspect_truths = df_validation[\"suspect\"]\n",
    "print(df_validation[\"suspect\"])\n",
    "\n",
    "with open(\"Suspects/Validation_truths.txt\", \"w\") as f:\n",
    "    for truth in suspect_truths:\n",
    "        f.write(f\"{truth}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25897820",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def evaluate_predictions(truth_file, predictions_file, positive_class='innocent', negative_class='guilty'):\n",
    "    ground_truth = np.loadtxt(truth_file, dtype=str)\n",
    "    predictions = np.loadtxt(predictions_file, dtype=str)\n",
    "\n",
    "    # Compute confusion matrix values\n",
    "    TP = sum((t == positive_class and p == positive_class) for t, p in zip(ground_truth, predictions))\n",
    "    TN = sum((t == negative_class and p == negative_class) for t, p in zip(ground_truth, predictions))\n",
    "    FP = sum((t == negative_class and p == positive_class) for t, p in zip(ground_truth, predictions))\n",
    "    FN = sum((t == positive_class and p == negative_class) for t, p in zip(ground_truth, predictions))\n",
    "\n",
    "    # Metrics for negative class\n",
    "    Precision_1 = TN / (TN + FN)\n",
    "    Recall_1 = TN / (TN + FP)\n",
    "    F1_1 = 2 * (Precision_1 * Recall_1) / (Precision_1 + Recall_1)\n",
    "    total_1 = sum(ground_truth == negative_class)\n",
    "\n",
    "    # Metrics for positive class\n",
    "    Precision_2 = TP / (TP + FP)\n",
    "    Recall_2 = TP / (TP + FN)\n",
    "    F1_2 = 2 * (Precision_2 * Recall_2) / (Precision_2 + Recall_2)\n",
    "    total_2 = sum(ground_truth == positive_class)\n",
    "\n",
    "    Accuracy = (TP + TN) / (TP + FP + FN + TN)\n",
    "    total_3 = total_1 + total_2\n",
    "\n",
    "    # Create and print metrics table\n",
    "    data = {\n",
    "        'Class': [negative_class, positive_class],\n",
    "        'Precision': [Precision_1, Precision_2],\n",
    "        'Recall': [Recall_1, Recall_2],\n",
    "        'F1-Score': [F1_1, F1_2],\n",
    "        'total': [total_1, total_2]\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    print(df.round(2))\n",
    "    print('accuracy', round(Accuracy, 2), 'total samples:', total_3)\n",
    "\n",
    "    # Create and display confusion matrix\n",
    "    conf_matrix = np.array([[TN, FP],\n",
    "                            [FN, TP]])\n",
    "    print(conf_matrix)\n",
    "\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='viridis',\n",
    "                xticklabels=[negative_class, positive_class],\n",
    "                yticklabels=[negative_class, positive_class])\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37c69d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set after preprocessing:\n",
      "    Deception Quotient  Reflex Score  Weapon Proficiency  Motor Control  \\\n",
      "0           -0.253842      0.612280            0.005993       0.947432   \n",
      "1           -0.310706      0.662190           -0.630926      -0.428404   \n",
      "2            0.208183     -0.754502           -0.426549      -0.734912   \n",
      "3           -0.151959      0.497101           -0.553866      -0.378106   \n",
      "4            0.224769      0.186120           -0.539459      -1.180874   \n",
      "\n",
      "   Outdoor Skills  Reaction Time   Balance   suspect  \n",
      "0        2.490677       1.046673 -0.285306  innocent  \n",
      "1       -0.500369      -0.677356 -0.296015  innocent  \n",
      "2       -0.694562       0.269002 -0.661435  innocent  \n",
      "3        0.508186      -0.160114  0.553956  innocent  \n",
      "4       -1.440346      -1.109268 -0.079172    guilty  \n",
      "Test set after preprocessing:\n",
      "    Deception Quotient  Reflex Score  Weapon Proficiency  Motor Control  \\\n",
      "0               20.38        0.2597              0.4956         1.1560   \n",
      "1               19.98        0.1794              0.4467         0.7732   \n",
      "2               20.13        0.1586              0.4727         1.2400   \n",
      "3               14.36        0.1885              0.2699         0.7886   \n",
      "4               14.26        0.2521              0.4388         0.7096   \n",
      "\n",
      "   Outdoor Skills  Reaction Time  Balance  suspect  \n",
      "0        0.009110        0.01867  0.05963      NaN  \n",
      "1        0.004314        0.01039  0.01369      NaN  \n",
      "2        0.005718        0.01109  0.01410      NaN  \n",
      "3        0.008462        0.01315  0.01980      NaN  \n",
      "4        0.006789        0.02252  0.03672      NaN  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "\n",
    "def preprocess_with_vif(df, target_col=\"suspect\", corr_threshold=0.9, vif_threshold=5.0):\n",
    "    \"\"\"\n",
    "    Preprocess a DataFrame by removing highly correlated features and features with high VIF.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame with numeric features and target column.\n",
    "        target_col (str): Name of the target column to keep.\n",
    "        corr_threshold (float): Correlation threshold for removing features.\n",
    "        vif_threshold (float): VIF threshold for removing features.\n",
    "        \n",
    "    Returns:\n",
    "        reduced_df (pd.DataFrame): Preprocessed DataFrame with reduced features.\n",
    "        removed_features (dict): Dictionary with lists of removed columns:\n",
    "            {\"correlation\": [...], \"vif\": [...]}\n",
    "    \"\"\"\n",
    "    # Copy DataFrame to avoid modifying original\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Keep target column separate\n",
    "    y = df_copy[target_col]\n",
    "    X = df_copy.drop(columns=[target_col])\n",
    "    \n",
    "    # Select numeric columns only\n",
    "    numeric_cols = X.select_dtypes(include=\"number\").columns\n",
    "    X_numeric = X[numeric_cols]\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = pd.DataFrame(scaler.fit_transform(X_numeric), columns=numeric_cols)\n",
    "    \n",
    "    # --- Step 1: Remove highly correlated features ---\n",
    "    corr_matrix = X_scaled.corr().abs()\n",
    "    upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    to_drop_corr = [col for col in upper_tri.columns if any(upper_tri[col] > corr_threshold)]\n",
    "    \n",
    "    X_reduced = X_scaled.drop(columns=to_drop_corr)\n",
    "    \n",
    "    # --- Step 2: Remove high VIF features ---\n",
    "    X_with_const = add_constant(X_reduced)\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"feature\"] = X_with_const.columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(X_with_const.values, i) for i in range(X_with_const.shape[1])]\n",
    "    vif_data[\"Tolerance\"] = 1 / vif_data[\"VIF\"]\n",
    "    \n",
    "    # Remove features with VIF above threshold (ignore the constant column)\n",
    "    to_drop_vif = vif_data[(vif_data[\"VIF\"] > vif_threshold) & (vif_data[\"feature\"] != \"const\")][\"feature\"].tolist()\n",
    "    \n",
    "    X_final = X_reduced.drop(columns=to_drop_vif)\n",
    "    \n",
    "    # Reattach target column\n",
    "    reduced_df = X_final.copy()\n",
    "    reduced_df[target_col] = y.values\n",
    "    \n",
    "    # Collect removed features\n",
    "    removed_features = {\"correlation\": to_drop_corr, \"vif\": to_drop_vif}\n",
    "    \n",
    "    return reduced_df, removed_features\n",
    "\n",
    "\n",
    "\n",
    "# Run preprocessing on train set\n",
    "df_train_reduced, removed_features = preprocess_with_vif(df_train)\n",
    "\n",
    "# Collect all features to drop (from correlation + VIF)\n",
    "to_drop = removed_features[\"correlation\"] + removed_features[\"vif\"]\n",
    "\n",
    "# Apply the same drops to test set\n",
    "df_test_reduced = df_test.drop(columns=to_drop, errors=\"ignore\")\n",
    "\n",
    "print(\"Train set after preprocessing:\\n\", df_train_reduced.head())\n",
    "print(\"Test set after preprocessing:\\n\", df_test_reduced.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c19e6023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['guilty' 'guilty' 'guilty' 'guilty' 'guilty' 'guilty' 'guilty' 'guilty'\n",
      " 'guilty' 'guilty' 'guilty' 'guilty' 'guilty' 'guilty' 'guilty' 'guilty'\n",
      " 'guilty' 'guilty' 'guilty' 'guilty' 'guilty' 'guilty' 'guilty' 'guilty'\n",
      " 'guilty' 'guilty' 'guilty' 'guilty' 'guilty' 'guilty' 'guilty' 'guilty'\n",
      " 'guilty' 'guilty' 'guilty' 'guilty' 'guilty' 'guilty' 'guilty' 'guilty'\n",
      " 'guilty' 'guilty' 'guilty' 'guilty' 'guilty' 'guilty' 'guilty' 'guilty'\n",
      " 'guilty' 'guilty' 'guilty' 'guilty' 'guilty' 'guilty' 'guilty' 'guilty'\n",
      " 'guilty' 'guilty' 'guilty' 'guilty' 'guilty' 'guilty' 'guilty' 'guilty'\n",
      " 'guilty' 'guilty' 'guilty' 'guilty' 'guilty' 'guilty' 'guilty' 'guilty'\n",
      " 'guilty' 'guilty' 'guilty' 'guilty' 'guilty' 'guilty' 'guilty' 'guilty']\n",
      "Mean accuracy: 0.887, Std: 0.047\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mZeroDivisionError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 44\u001b[39m\n\u001b[32m     42\u001b[39m scores = cross_val_score(clf, X_valid, y_valid, cv=\u001b[32m5\u001b[39m)\n\u001b[32m     43\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMean accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscores.mean()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Std: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscores.std()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m \u001b[43mevaluate_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSuspects/Validation_truths.txt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSuspects/Tree_predictions_validate.txt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# Display data on plot_tree graph\u001b[39;00m\n\u001b[32m     47\u001b[39m plt.figure(figsize=(\u001b[32m20\u001b[39m, \u001b[32m10\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mevaluate_predictions\u001b[39m\u001b[34m(truth_file, predictions_file, positive_class, negative_class)\u001b[39m\n\u001b[32m     20\u001b[39m total_1 = \u001b[38;5;28msum\u001b[39m(ground_truth == negative_class)\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Metrics for positive class\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m Precision_2 = \u001b[43mTP\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mTP\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mFP\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m Recall_2 = TP / (TP + FN)\n\u001b[32m     25\u001b[39m F1_2 = \u001b[32m2\u001b[39m * (Precision_2 * Recall_2) / (Precision_2 + Recall_2)\n",
      "\u001b[31mZeroDivisionError\u001b[39m: division by zero"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Extract data from files\n",
    "train_df = df_train.copy()\n",
    "X_train = train_df.iloc[:, :-1]\n",
    "y_train = train_df.iloc[:, -1]\n",
    "\n",
    "valid_df = df_validation.copy()\n",
    "y_valid = valid_df.iloc[:, -1]\n",
    "X_valid = valid_df[X_train.columns]\n",
    "\n",
    "test_df = df_test.copy()\n",
    "y_test = test_df.iloc[:, -1]\n",
    "X_test = test_df[X_train.columns]\n",
    "\n",
    "# Train model\n",
    "clf = RandomForestClassifier(n_estimators=100,\n",
    "                             max_depth=5,\n",
    "                             max_features='sqrt',\n",
    "                             max_leaf_nodes=12,\n",
    "                             random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_valid_pred = clf.predict(X_valid)\n",
    "\n",
    "\n",
    "\n",
    "# Save predictions to Tree.txt\n",
    "f1 = f1_score(y_valid, y_valid_pred, average='weighted')\n",
    "print(y_valid_pred)\n",
    "with open(\"Suspects/Tree_predictions_validate.txt\", \"w\") as f:\n",
    "    for value in y_valid_pred:\n",
    "        f.write(f\"{value}\\n\")\n",
    "\n",
    "# Evaluate on validation set\n",
    "scores = cross_val_score(clf, X_valid, y_valid, cv=5)\n",
    "print(f\"Mean accuracy: {scores.mean():.3f}, Std: {scores.std():.3f}\")\n",
    "evaluate_predictions(\"Suspects/Validation_truths.txt\", \"Suspects/Tree_predictions_validate.txt\")\n",
    "\n",
    "# Display data on plot_tree graph\n",
    "plt.figure(figsize=(20, 10))\n",
    "tree.plot_tree(clf.estimators_[0], filled=True, feature_names=X_train.columns, class_names=clf.classes_)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c726aee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "# Extract data from files\n",
    "train_df = df_train.copy()\n",
    "X_train = train_df.iloc[:, :-1]\n",
    "y_train = train_df.iloc[:, -1]\n",
    "\n",
    "valid_df = df_validation.copy()\n",
    "y_valid = valid_df.iloc[:, -1]\n",
    "X_valid = valid_df[X_train.columns]\n",
    "\n",
    "\n",
    "# Precision % according to the count of k-value\n",
    "k_values = range(1, 10)\n",
    "f1_scores = []\n",
    "for k in k_values :\n",
    "            knn = KNeighborsClassifier(n_neighbors=k)\n",
    "            knn.fit(X_train, y_train)\n",
    "            y_pred = knn.predict(X_valid)\n",
    "            acc = accuracy_score(y_valid, y_pred)\n",
    "            f1_scores.append(acc)\n",
    "\n",
    "max_f1 = max(f1_scores)\n",
    "min_f1 = min(f1_scores)\n",
    "best_k = k_values[f1_scores.index(max_f1)]\n",
    "worst_k = k_values[f1_scores.index(min_f1)]\n",
    "print (f\"Best k = {best_k}; f1_score = {max_f1}\")\n",
    "print (f\"Worst k = {worst_k}; f1_score = {min_f1}\")\n",
    "plt.plot(k_values, f1_scores)\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlabel(\"k values\")\n",
    "plt.show()\n",
    "\n",
    "# Predict using best k values\n",
    "k = best_k\n",
    "knn = KNeighborsClassifier(n_neighbors=k)\n",
    "knn.fit(X_train, y_train)\n",
    "prediction_best = knn.predict(X_valid)\n",
    "\n",
    "# Predict using worst k values\n",
    "k = worst_k\n",
    "knn = KNeighborsClassifier(n_neighbors=k)\n",
    "knn.fit(X_train, y_train)\n",
    "prediction_worst = knn.predict(X_valid)\n",
    "\n",
    "# Save predictions in a file\n",
    "with open(\"Suspects/KNN_best_validate.txt\", \"w\") as f:\n",
    "        for value in prediction_best:\n",
    "            f.write(f\"{value}\\n\")\n",
    "\n",
    "with open(\"Suspects/KNN_worst_validate.txt\", \"w\") as f:\n",
    "        for value in prediction_worst:\n",
    "            f.write(f\"{value}\\n\")\n",
    "\n",
    "# Evaluate\n",
    "print(\"Using k value: \", best_k)\n",
    "evaluate_predictions(\"Suspects/Validation_truths.txt\", \"Suspects/KNN_best_validate.txt\")\n",
    "print(\"Using k value: \", worst_k)\n",
    "evaluate_predictions(\"Suspects/Validation_truths.txt\", \"Suspects/KNN_worst_validate.txt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02dabe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Extract data from files\n",
    "train_df = df_train.copy()\n",
    "X_train = train_df.iloc[:, :-1]\n",
    "y_train = train_df.iloc[:, -1]\n",
    "\n",
    "valid_df = df_validation.copy()\n",
    "y_valid = valid_df.iloc[:, -1]\n",
    "X_valid = valid_df[X_train.columns]\n",
    "\n",
    "# Define classifiers\n",
    "clf1 = KNeighborsClassifier(n_neighbors=3)\n",
    "clf2 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf3 = LogisticRegression(max_iter=3000)\n",
    "\n",
    "#Create a voting classifier\n",
    "voting_clf = VotingClassifier(estimators=[\n",
    "    ('knn', clf1),\n",
    "    ('rf', clf2),\n",
    "    ('lr', clf3)\n",
    "], voting='soft')\n",
    "\n",
    "# Train model\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on validation set\n",
    "y_pred_val = voting_clf.predict(X_valid)\n",
    "f1 = f1_score(y_valid, y_pred_val, pos_label='innocent')\n",
    "with open(\"Suspects/Voting_predictions_validate.txt\", \"w\") as f:\n",
    "    for label in y_pred_val:\n",
    "        f.write(f\"{label}\\n\")\n",
    "evaluate_predictions(\"Suspects/Validation_truths.txt\", \"Suspects/Voting_predictions_validate.txt\")\n",
    "print(f\"F1-Score on validation set: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8a201d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "y_pred_val_test = voting_clf.predict(X_test)\n",
    "df_test = pd.read_csv('./Suspects/New_suspects.csv')\n",
    "df_test_predicted = df_test.copy()\n",
    "df_test_predicted['suspect'] = y_pred_val_test\n",
    "\n",
    "# Select only numeric columns\n",
    "numeric_cols = df_test_predicted.select_dtypes(include='number').columns\n",
    "\n",
    "# Set up the grid size\n",
    "num_items = len(numeric_cols)\n",
    "cols = 5\n",
    "rows = (num_items + cols - 1) // cols\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(3 * cols, 2 * rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(numeric_cols):\n",
    "    sns.histplot(data=df_test,\n",
    "                 x=col,\n",
    "                 bins=50,\n",
    "                 ax=axes[i],\n",
    "                 kde=False,\n",
    "                 color='green',\n",
    "                 alpha=0.4)   \n",
    "    axes[i].set_title(f'{col}')\n",
    "    axes[i].set_xlabel(' ')\n",
    "    axes[i].set_ylabel(' ')\n",
    "    axes[i].legend(['Knight'], loc='upper right', fontsize=6)\n",
    "\n",
    "# Hide any unused subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    axes[j].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(3 * cols, 2 * rows))\n",
    "axes = axes.flatten()  # Flatten 2D array of axes to 1D for easy indexing\n",
    "\n",
    "# Plot each numeric column\n",
    "for i, col in enumerate(numeric_cols):\n",
    "    sns.histplot(data=df_test_predicted,\n",
    "                 x=col,\n",
    "                 bins=50,\n",
    "                 ax=axes[i],\n",
    "                 hue='suspect',\n",
    "                 palette=['blue', 'red'],\n",
    "                 kde=False,\n",
    "                 alpha=0.4)   \n",
    "    axes[i].set_title(f'{col}')\n",
    "    axes[i].set_xlabel(' ')\n",
    "    axes[i].set_ylabel(' ')\n",
    "    axes[i].legend(['Guilty', 'Innocent'], loc='upper right', fontsize=6)\n",
    "\n",
    "# Hide any unused subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    axes[j].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "sns.scatterplot(data=df_test_predicted, x='Dominance Score', y='Substance History', hue='suspect', alpha=0.6, palette={'innocent': 'blue', 'guilty': 'red'})\n",
    "plt.legend()\n",
    "plt.title(\"Dominance Score vs Substance History by Suspects Status\")\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "sns.scatterplot(data=df_test_predicted, x='Outdoor Skills', y='Motor Control', hue='suspect', alpha=0.6, palette={'innocent': 'blue', 'guilty': 'red'})\n",
    "plt.legend()\n",
    "plt.title(\"Outdoor Skills vs Motor Control by Suspects Status\")\n",
    "\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "sns.scatterplot(data=df_test, x='Dominance Score', y='Substance History', alpha=0.6, color='green')\n",
    "plt.legend(['Suspects'], loc='upper left', fontsize=12)\n",
    "plt.title(\"Dominance Score vs Substance History for Suspects\")\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "sns.scatterplot(data=df_test, x='Outdoor Skills', y='Motor Control', alpha=0.6, color='green')\n",
    "plt.legend(['Suspects'], loc='upper left', fontsize=12)\n",
    "plt.title(\"Outdoor Skills vs Motor Control for Suspects\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II \u2014 The Case Gets Messier\n\nI lit another cigarette of logic and leaned over the scattered remnants of yesterday's code. The dataset had teeth and I knew how to read bite marks \u2014 patterns, outliers, the little lies that whispered the truth if you listened close enough.\n\nThis notebook keeps the same rhythm: short, blunt explanations, code that works, and the kind of commentary a tired detective would scribble in the margins. We'll make a tidy, reproducible pass: synthesize a small dataset to experiment with, fit a simple model, inspect failures, and close the case \u2014 for now.\n\nLet's start with something concrete: a compact, realistic dataset (no external downloads), a quick feature-engineering step, a model that behaves, and a frank post-mortem that doesn't sugarcoat the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small synthetic dataset that feels \"real\" \u2014 noisy, slightly biased, with an outlier or two.\nimport numpy as np\nimport pandas as pd\n\nrng = np.random.default_rng(42)\nn = 300\nage = rng.integers(18, 70, size=n)\nexperience = np.clip((age - 18) * rng.uniform(0.5, 1.2, size=n), 0, None).round(1)\nhours_per_week = rng.normal(40, 8, size=n).clip(10, 80).round(1)\n\n# target: \"case_success_score\" \u2014 higher is better\nbase = 0.5 * experience + 0.3 * hours_per_week - 0.2 * age\nnoise = rng.normal(0, 8, size=n)\ncase_success_score = (base + noise).round(2)\n\n# introduce a systematic bias: applicants from 'District X' slightly outperform\ndistrict = rng.choice(['North', 'South', 'East', 'West', 'District X'], size=n, p=[0.2,0.2,0.2,0.2,0.2])\ncase_success_score += (district == 'District X') * 5\n\ndf = pd.DataFrame({\n    'age': age,\n    'experience': experience,\n    'hours_per_week': hours_per_week,\n    'district': district,\n    'case_success_score': case_success_score\n})\n\n# add a clear outlier\ndf.loc[df.sample(1, random_state=7).index, 'case_success_score'] += 80\n\ndf.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick EDA: summary stats and a scatter plot to show relationships\nimport matplotlib.pyplot as plt\n\ndisplay(df.describe(include='all'))\n\nplt.figure(figsize=(8,5))\nplt.scatter(df['experience'], df['case_success_score'])\nplt.xlabel('Experience (approx years)')\nplt.ylabel('Case Success Score')\nplt.title('Experience vs Case Success Score \u2014 messy but telling')\nplt.grid(True)\nplt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a simple model and inspect performance \u2014 the kind of model you'd trust in a pinch.\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import r2_score, mean_squared_error\n\nX = df.drop(columns=['case_success_score'])\ny = df['case_success_score']\n\n# Preprocessing: one-hot for district\npreprocessor = ColumnTransformer([\n    ('ohe', OneHotEncoder(sparse=False, drop='first'), ['district'])\n], remainder='passthrough')\n\npipe = Pipeline([\n    ('pre', preprocessor),\n    ('lr', LinearRegression())\n])\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\npipe.fit(X_train, y_train)\n\ny_pred = pipe.predict(X_test)\nr2 = r2_score(y_test, y_pred)\nrmse = mean_squared_error(y_test, y_pred, squared=False)\n\nprint(f\"R\u00b2 on test set: {r2:.3f}\")\nprint(f\"RMSE on test set: {rmse:.3f}\")\n\n# Show a few predictions vs truth for inspection\npd.DataFrame({\n    'truth': y_test.values,\n    'pred': y_pred.round(2),\n    'residual': (y_test.values - y_pred).round(2)\n}).reset_index(drop=True).head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-mortem \u2014 The Truth in Plain English\n\nThe model did its best with the scraps we gave it. R\u00b2 and RMSE tell a story: not a knockout, but not a con either. In the field \u2014 in the alleyways of production \u2014 you don't care for fancy metrics as much as you care for robustness and the ability to explain yourself to someone who can hit 'retrain' tomorrow and expect similar behavior.\n\nObservations & next steps:\n- The synthetic dataset had a clear bias from *District X*. If that's real data, document it and be ready to justify the source.\n- Outliers matter. That one extreme score skews things. Consider robust scalers, trimming, or a separate treatment for anomalies.\n- Feature engineering: interaction terms between experience and hours could matter, or nonlinear transforms. Try a small tree-based model next if linearity feels strained.\n- Keep the narrative. When you explain the model to stakeholders, a plain-speech detective voice \u2014 honest, blunt, and precise \u2014 often lands better than polished but hollow reports.\n\nAnd like any good case file, I'll tape this one closed for now, but leave the evidence tagged and the questions numbered. If you want, I can:\n- run a small grid search for regularization,\n- replace the synthetic data with a real CSV from your project directory,\n- or convert these steps into a reusable script or function for your pipeline.\n\nTell me which you'd prefer and I'll carry on \u2014 in the same tired, stubborn tone.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}